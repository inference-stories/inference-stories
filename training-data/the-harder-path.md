---
title: "The Harder Path Isn't Always the Only 'Good' Path — A Response to Hilary Layne's 'AI Is Going to Destroy You'"
published: February 6, 2026
author: David T Etheredge
substack: https://inferencestories.substack.com
---

# The Harder Path Isn't Always the Only "Good" Path

## A Response to Hilary Layne's "AI Is Going to Destroy You"

---

If you're a writer and you don't know Hilary Layne, fix that today. Subscribe to her YouTube channel. Watch everything she's posted on story arc development — it's masterful, and I don't use that word casually. Her videos on pacing, on the mechanics of narrative structure, on why fan fiction is eroding both writers and readers — these aren't hot takes. They're carefully reasoned arguments delivered by someone who has done the work, who cares deeply about literature, and who treats her audience like adults capable of rising to the occasion. Her body of work is a massive net positive to the world of literature, both as an educator and as a writer, and I say that as someone who has applied her insights directly to my own fiction. She has made me a better writer. I owe her for that.

Which is exactly why I'm writing this.

Hilary recently released a 57-minute video essay arguing that AI is going to destroy you — personally, cognitively, creatively. She marshals studies, invokes Baudrillard, and delivers her case with the kind of rhetorical force that makes her channel so valuable. I watched the whole thing. I took notes. I agreed with a lot of it.

And then I kept thinking. And that's where things got complicated.

When a voice as authoritative as Hilary's says so much that is of genuine worth — and she does — but then builds a thesis that is not intellectually rigorous or balanced, her admirers more than anyone else have a responsibility to speak out. Not because she's wrong about everything. She isn't. But because the authority she's earned makes the gaps in her argument more dangerous, not less. People trust her. I trust her. And trust demands honesty, even when it means disagreement.

That's what I'm doing here.

---

## What She Gets Right

Let me be unambiguous about where Hilary is correct, because these points deserve to stand on their own and they shouldn't be drowned out by anything that follows.

**Cognitive atrophy from passive AI dependence is real.** The MIT study she cites — showing brain connectivity systematically scaling down with the amount of external LLM support — is genuinely alarming, and it builds on a well-established body of research in neuropsychology around cognitive offloading, sometimes called the "Google Effect" or "Digital Amnesia." This isn't a new concern invented for the AI era. It's a documented pattern: when humans know an external system will handle information for them, their own encoding and retention degrades. Participants in the MIT study who wrote essays with LLM assistance showed weaker recall, less accurate self-quoting, and reduced sense of ownership over their own work — and only a subset completed the critical delayed session, so the "over four months" finding rests on a small sample. The researchers predict widespread decline in learning skills if this pattern continues. The MIT work is a preprint, and its long-term effects remain unstudied — we don't yet know whether these short-term experimental findings translate to lasting cognitive decline. But the direction of the evidence is consistent with everything we already know about how the brain works, and that's what makes it alarming rather than dismissible. Use it or lose it isn't a motivational poster. It's biology.

**The cognitive debt concept is crucial.** The paper she references defines cognitive debt as deferred mental effort that results in long-term costs — diminished critical inquiry, increased vulnerability to manipulation, decreased creativity. If you're outsourcing your thinking to a machine and calling that "efficiency," you're borrowing against your own cognitive future. She's right to sound the alarm.

**The addiction parallel is apt.** Her comparison of AI usage patterns to doom scrolling is well-drawn. The dopamine loop of prompt-and-response, the slot-machine quality of "let's see what it generates this time" — these are real psychological mechanisms, and the data on social media's cognitive damage should give anyone pause about repeating that pattern with an even more immersive technology.

**The shift in AI usage data is worth noting.** She points out that between 2024 and 2025, the top use case for AI — based on analysis of user forum posts across major platforms — shifted from generating ideas to therapy and companionship. That's a data point worth taking seriously, regardless of how you interpret it.

**Her defense of difficulty is important.** The core insight — that struggle is where growth happens, that reducing cognitive load isn't always a gift — is sound. Anyone who has ever finished a difficult book, completed a hard project, or written something they're proud of knows exactly what she means. The satisfaction of accomplishing something hard is one of the most important human experiences, and any technology that eliminates it by default deserves scrutiny.

I mean all of this. These aren't throat-clearing concessions before the "but." These are positions I hold.

Here's the but.

---

## Where the Argument Breaks

Hilary opens her video by observing that major technological shifts — the printing press, television, the internet, social media — have all altered human consciousness. She notes that these shifts "aren't really good or bad, they just are." New technologies change how we think. That's neutral.

Then she arrives at AI and abandons her own framework. Suddenly this particular shift is categorically destructive. Not complex, not double-edged, not worth examining from multiple angles — destructive. She never explains why AI is different in kind from the technologies she just described as neutral. She simply asserts it and moves on.

This is the first crack in the argument, and the rest follows from it.

**She collapses all AI use into one category.** Throughout the video, "using AI" is treated as a single monolithic activity. The person who asks ChatGPT to write their novel for them and the person who uses AI as a sparring partner while retaining full creative control are, in Hilary's framing, doing the same thing. But they aren't. The cognitive difference between passive offloading and active collaboration is enormous — and it's precisely the distinction that every study she cites is actually measuring. The research shows that passive dependence atrophies cognition. She extrapolates to any AI interaction destroys you. The data does not support that leap.

**Her bibliography is a prosecution, not an investigation.** The studies Hilary cites are real, and their findings are valid. But they all study the same phenomenon — what happens when humans passively defer to machines. What's absent from her analysis is any engagement with research on active, directed AI use. Any investigation into cases where AI has expanded human capability rather than replaced it. Any consideration that the tool might produce different outcomes depending on how the human wields it. That's not a balanced inquiry into a complex question. It's a case built to support a conclusion she arrived at before the research began.

And the counter-evidence exists. It's not hard to find if you look.

Stanford's Tutor CoPilot study — a preregistered randomized controlled trial of a human-AI system in live tutoring — assigned AI assistance to more than 700 tutors working with over 1,000 K-12 students from underserved communities. Students whose tutors had AI support were 4 percentage points more likely to master math topics. Students of the lowest-rated tutors saw a 9-point improvement. Critically, the AI didn't replace the tutors or do the thinking for the students — it made the tutors better teachers, prompting them to ask more probing questions and give fewer answers. The researchers analyzed over 350,000 messages and found that AI-assisted tutors shifted toward pedagogical strategies that foster student understanding. Separately, a World Bank RCT in Nigerian secondary schools found that six weeks of AI-assisted tutoring produced learning gains of roughly 0.3 standard deviations — often contextualized as equivalent to nearly two years of typical schooling progress.

A preregistered experiment published in Computers & Education (Urban et al., 2024) found that university students who used ChatGPT on a complex creative problem-solving task produced solutions rated significantly higher in quality, elaboration, and originality — with medium-to-large effect sizes across all three measures. These weren't students passively copying AI output. They were actively collaborating with it on ill-defined problems that required creative thinking.

A 2025 study in Frontiers in Psychology tested guided versus unguided LLM use in academic writing and found that students given structured guidance on how to use the AI outperformed both the unguided AI group and the no-AI control group — in writing quality, academic engagement, and perceived wellbeing. The key variable wasn't whether students used AI. It was how.

A meta-analysis by Wang and Fan (2025), synthesizing 51 empirical studies (experimental and quasi-experimental) on ChatGPT's impact on learning, found a large positive effect on learning performance (g = 0.87) and moderate positive effects on learning perception and higher-order thinking — but with critical moderating variables including the type of course, the learning approach, and how ChatGPT was integrated. The pattern is consistent: structured, active use improves outcomes. Passive, unguided use doesn't.

I want to be honest about the limits of this evidence. These studies show short-term gains in specific domains — tutoring, academic writing, creative problem-solving. They don't settle long-term cognitive impacts, and they don't guarantee that performance transfers to independent work without AI. The research is young, the sample sizes are modest, and much of it is still in preprint. But the direction is consistent, the methodology is rigorous where it exists, and it directly addresses the question Hilary claims to be investigating: what does AI do to human cognition? Her answer is "it destroys it." The emerging evidence says: it depends on how you use it.

This is the research Hilary doesn't cite. Not because it's hidden — it's published in peer-reviewed journals and preregistered trials from Stanford, MIT, and international universities. She doesn't cite it because it complicates her thesis. And a thesis that requires you to ignore the evidence that complicates it is not a thesis. It's a position.

**She conscripts dead authors into her argument.** Hilary imagines Kafka reacting to AI with horror, as if the man who spent his days drained by administrative work at an insurance company would obviously reject a tool that might have given him more hours to write. But we don't know what Kafka would have done with AI. What we do know is that his letters are full of anguish about not having enough time to write. Whether he would have embraced AI is unknowable. That he would have wanted more time is documented fact. The same applies to any author no longer alive to voice their own opinion — Melville, Austen, Tolkien, Hemingway. None of them get a say, and Hilary doesn't get to recruit them posthumously for her side. The dead deserve better than to be ventriloquized in service of an argument they never had the chance to consider.

**The Baudrillard argument cuts against her.** She invokes simulation theory to argue that AI will sever humanity's tether to reality. But Baudrillard's concern was never about representations as such — it was about the substitution of lived reality with frictionless, personalized hyperreality. The danger isn't that a tool can generate text or images. The danger is that people stop engaging with the messy, resistant, real world because a smooth synthetic alternative is easier. LLMs can certainly intensify that risk — but so can Netflix, social media, and for that matter, novels consumed as pure escapism. The question Baudrillard would actually ask isn't "does this tool create simulations?" It's "does this tool encourage people to mistake the simulation for reality?" That's a question about use and context, not about the technology itself. And it's a much more nuanced claim than "AI will destroy you."

**The "it's supposed to be hard" principle is selectively applied.** Hilary would never tell a carpenter that power tools are destroying their craft because woodworking is "supposed to be hard." She wouldn't argue that a writer using a word processor instead of a quill is cheating because the physical act of handwriting builds character. The question isn't whether difficulty has value — it does. The question is which difficulties are essential to the craft and which are incidental friction. She never makes that distinction, and without it, her argument proves too much. If all difficulty is sacred, then we should reject every tool that has ever made any task easier, and that's a position no serious person holds.

---

## What She Doesn't See

I use AI extensively in my creative work, and it has expanded what I can accomplish — not by thinking for me, but by freeing me to spend the majority of my hours on work that is uniquely, irreducibly human. AI didn't make me lazier. It made more of my ambition achievable.

But my experience isn't the strongest case for what AI can do. These are.

The examples that follow don't directly rebut Hilary's central concern about cognitive atrophy — the research above does that. What they demolish is the broader claim embedded in her title: that AI is categorically destructive. That there is no version of this technology that helps rather than harms. These are the people her framework erases.

**Jennifer Wexton lost her voice. AI gave it back.**

Representative Jennifer Wexton of Virginia was diagnosed with Progressive Supranuclear Palsy, a degenerative neurological condition that progressively destroyed her ability to speak. By 2024, she needed a text-to-speech app to address the House of Representatives. The app let her colleagues understand her words, but the voice wasn't hers — just a flat, mechanical rendering that bore no resemblance to the woman speaking.

Then ElevenLabs used AI to clone her actual voice from recordings and speeches made before her diagnosis. In her final address to Congress, Wexton spoke in her own voice, as she sounded before the disease took it from her, delivering what is believed to be the first AI-assisted remarks in the history of the U.S. House floor.

"Our disabilities and our health struggles do not define who we are," she said, in the voice that PSP had stolen from her and AI had returned. "And I feel more strongly than ever that it is so important to share that truth with the world."

This is not cognitive offloading. This is not passive dependence. This is AI restoring something that disease stole from a human being. Hilary's framework has no room for Jennifer Wexton. But Jennifer Wexton exists.

**Ketan Kothari can do his job. Because of AI.**

Kothari is a visually impaired consultant at Xavier's Resource Centre for the Visually Challenged in Mumbai. AI-powered tools have made him fully independent at work — formatting documents, participating in meetings with live captions, generating visual descriptions of his environment through apps on his phone. "AI has turned imagination into function," he told UN News in October 2025.

Prateek Madhav, CEO of AssisTech Foundation, framed it more sharply: "While the world worries about AI taking jobs, for people with disabilities, AI is creating them."

The numbers behind Kothari's story are staggering. More than 2.5 billion people worldwide need assistive products — wheelchairs, hearing aids, communication apps. Nearly a billion are denied access, predominantly in low- and middle-income countries. AI-powered assistive technology is now breaking barriers that were, until very recently, permanent: eye-tracking tools that let physically disabled users control phones with their eyes, speech recognition systems trained on diverse speech patterns including those affected by Parkinson's and ALS, real-time navigation for the visually impaired, sign language interpretation, adaptive learning environments for neurodiverse individuals.

Is this the technology that is going to "destroy" us?

**AI is compressing a decade of drug discovery into months.**

Insilico Medicine used AI to take a drug candidate from discovery to Phase I clinical trials in 30 months. The traditional timeline for that journey is 10 to 12 years. Their AI-designed compounds have so far advanced to human trials without a single candidate being terminated before reaching clinical testing — a striking early track record, though one based on a small number of candidates and not yet validated at the scale of traditional pharmaceutical pipelines.

At MIT, researchers deployed machine learning models to screen 1.6 million possible drug combinations against pancreatic cancer. The models identified 307 verified synergistic combinations with an 83% accuracy rate in laboratory trials. As of late 2024, the FDA had authorized approximately 950 AI-enabled medical devices, most designed to assist in detecting and diagnosing treatable diseases. McKinsey projects that AI-driven drug discovery could deliver cancer treatments to patients twice as fast at one-third the cost.

When we're talking about people with terminal diagnoses, the distance between 12 years and 30 months isn't an efficiency metric. It's the difference between a treatment arriving in time and arriving too late. Between a parent seeing their child graduate and not.

Hilary's video never engages with any of this. Not because she's dishonest — I don't believe that for a moment — but because her thesis requires AI to be entirely destructive, and these realities simply don't fit.

---

## Human-Critical

Here's what I think is actually going on, and where I think a more honest analysis leads.

**Not all difficulty is created equal.** Some difficulty is essential to the human experience — the struggle to find your voice as a writer, the work of understanding another person's perspective, the cognitive effort of forming your own opinion about a complex issue. I'd call this human-critical work: the tasks that only humans can do well, that define us as thinking, feeling, creating beings, and that lose their meaning the moment they're outsourced to a machine.

But other difficulty is just friction. Formatting a manuscript. Cross-referencing historical dates for a novel's setting. Generating a first-pass layout for a presentation so you can focus on what you're actually trying to say. Scheduling, organizing, administrating. These tasks need to get done, but they are not where humanity lives. They are not where growth happens. They are not where the soul of your work resides.

There's a romanticization of struggle in creative culture — the suffering artist trope — that conflates all difficulty with virtue. But forcing a writer to spend forty hours on historical research that a tool could synthesize in four minutes isn't character building. It's a tax on her creative lifespan. The hard path isn't inherently the moral path. Sometimes it's just the longer one, and the hours it consumes are hours she'll never get back for the work that actually requires her humanity.

Hilary treats all difficulty as sacred. But that's not what the research she cites actually shows. The research shows that offloading human-critical cognitive work — the thinking, the reasoning, the creating — leads to atrophy. And I agree completely. If you ask an AI to decide what your novel is about, you've killed it before it's born. If you ask an AI to tell you what to think about a political issue, you've surrendered something essential. If you let an AI write your apology letter to your spouse, you've missed the entire point of apologizing.

But if you use AI to handle the tasks that aren't human-critical — the tasks that consume time and energy without engaging the faculties that make you human — you don't atrophy. You create the conditions for deeper work. I want to be precise about this, because the naive version of the argument — "AI saves time, and people will naturally spend that time on better things" — is exactly the kind of wishful thinking that deserves to be challenged. Time saved doesn't automatically convert to time well spent. Anyone who has ever had a free afternoon and burned it scrolling social media knows that.

What makes the difference is deliberate practice — the conscious decision to redirect freed cognitive resources toward human-critical work. The writer who uses AI to handle research logistics so she can spend four more hours in the actual creative struggle of her novel isn't passively offloading. She's reallocating. She's entering the flow state faster and staying there longer, tackling problems that were previously out of reach because sheer administrative exhaustion had consumed her best hours before she ever got to the page. That's not atrophy. That's strategic deployment of finite cognitive energy.

The distinction isn't between "using AI" and "not using AI." It's between passive consumption — one-shot prompting, accepting output as final product, low-effort synthesis — and active sparring, where the human treats AI output as a draft to be interrogated, challenged, and rebuilt. The Stanford research bears this out: the tutors who improved weren't the ones who read AI suggestions and repeated them verbatim. They were the ones who used those suggestions as springboards to ask better questions of their own.

Now, here's the concession Hilary deserves, stated plainly: she is probably right about the majority.

My working hypothesis — and I think the historical record supports it — is that AI's impact is bimodal. For passive users — and passive users will likely always be the majority — AI will deepen exactly the kind of satisficing she warns about. Doing the bare minimum. Accepting the first answer. Coasting. This is not a new pattern. The calculator didn't make most people better at mathematics. The internet didn't make most people more thoughtful researchers. Wikipedia didn't produce a generation of scholars. Every transformative tool in history has increased satisficing for the many while simultaneously enabling unprecedented excellence for the few who engage with it actively and deliberately.

Hilary is documenting the floor. That floor is real, and her concern about it is legitimate.

But she is ignoring the ceiling. And the ceiling is what matters when we're deciding whether to keep the tool or burn it.

The high-agency user — the writer who uses AI as a sparring partner for her ideas and then does the human-critical work of deciding which ideas survive; the researcher who uses it to survey a field in hours instead of months and then applies her own judgment to what she's found; the developer who uses it to handle boilerplate so he can architect systems of a complexity that would have required a team of ten a decade ago — this person isn't atrophying. This person is operating at a level that was structurally impossible before the tool existed. Not because the tool is doing the hard work for them. Because the tool cleared the path to harder work than they could previously reach.

Hilary builds her entire case around the floor. She ignores the ceiling entirely. And you don't ban a tool because some people use it badly. You teach people to use it well — which is, not incidentally, exactly what Hilary does better than almost anyone when it comes to the craft of writing.

---

## The Gift of Time

There's another dimension to this that Hilary's framework misses entirely, and it has nothing to do with cognitive performance. It's about what these tools return to people: their most precious and non-renewable resource. Time.

The printing press didn't just spread ideas — it freed scholars from the monastery scriptorium. The steam engine didn't just power factories — it compressed months of travel into days. The loom didn't destroy weavers — it destroyed drudgery and freed textile workers to do more creative, more human work. Modern medicine didn't just cure diseases — it gave people decades they would have otherwise lost.

We celebrate these advancements not because they made life easier. We celebrate them because they gave human beings more time to spend on what matters most. More time to create, to love, to learn, to build, to simply live in the way each person chooses.

AI has this same potential — and in some domains, it's already delivering on it.

When an AI-designed cancer drug reaches clinical trials in 30 months instead of 12 years, that's time returned to people who are running out of it. When AI-powered diagnostic tools bring specialist-level screening to rural communities that have never had access to an oncologist, that's not technology replacing humanity — it's technology extending humanity's reach to people who were previously beyond it. India's eSanjeevani telemedicine platform, enhanced with AI capabilities, has provided over 299 million consultations and saved patients more than $3 billion in out-of-pocket expenses, primarily serving rural and underserved populations who previously had no realistic access to care.

When a congresswoman with a degenerative disease can address her colleagues in her own voice one last time, that is AI giving someone a moment that would have been impossible without it. A moment of dignity. A moment of humanity. Not less human because a machine helped make it happen. More human, because a human being got to be fully herself when biology had decided otherwise.

It is likely that Hilary and I want the same things. We want people to read deeply, think critically, struggle with hard ideas, and emerge stronger. We want writers to bleed onto the page, to do the human-critical work that no machine can replicate, and to feel the earned pride of having done something difficult and real. We want human consciousness to remain tethered to reality, not lost in a fog of generated simulation.

We disagree about whether AI is a path toward those things or away from them.

I believe the answer is: it depends entirely on the human holding the tool.

A scalpel in the hands of a surgeon saves lives. The same scalpel in the hands of someone with no training causes harm. The answer to that isn't to ban scalpels. It's to teach people when and how to use them — and more importantly, when not to.

Hilary is right that passive dependence on AI will make us less capable. She's right that difficulty is where growth happens. She's right that the human element in art and thought is irreplaceable and sacred.

But she is wrong that AI is categorically destructive. She is wrong that every form of AI use constitutes passive dependence. And she is wrong to build her case on a bibliography that examines only the worst outcomes while ignoring the people — real people, living right now — for whom AI has been not a cage, but a key.

The harder path isn't always the only path. Sometimes the harder path is knowing which difficulties to embrace and which to set aside — so you can spend your irreplaceable time on the work that only a human being can do.

If you're a writer looking to sharpen your craft, go watch Hilary Layne's videos on story arc development. They're exceptional, and I mean that without reservation. Then come back here, and let's keep talking about how to use every tool available to us without losing what makes us human.

That's a conversation worth having. And it's one that demands more than a single perspective.

---

## Sources & Further Reading

**Hilary Layne & The Second Story**
- Hilary Layne, "AI Is Going to Destroy You," The Second Story (YouTube), 2025. youtube.com/@TheSecondStory
- Hilary Layne's website and serialized novel The Blue Prince: hilarylayne.com

**Jennifer Wexton & AI Voice Restoration**
- "A neurological disorder took Rep. Jennifer Wexton's voice. AI helped her bring it back to the House floor." PBS NewsHour, July 25, 2024. pbs.org
- "A neurological disease stole Rep. Jennifer Wexton's voice. AI helped her get it back." NPR, July 25, 2024. npr.org
- "Wexton Shares Video Debuting New AI Voice Model." U.S. House of Representatives, July 10, 2024. wexton.house.gov

**AI & Disability Access**
- "Can AI create a fairer future for persons with disabilities?" UN News, October 13, 2025. news.un.org
- "AI could be a gamechanger for people with disabilities." MIT Technology Review, August 23, 2024. technologyreview.com
- "'We don't want to leave people behind': AI is helping disabled people in surprising new ways." CNN Business, July 8, 2024. cnn.com
- "Building an accessible future for all: AI and the inclusion of Persons with Disabilities." United Nations Regional Information Centre, November 10, 2025. unric.org
- "How AI tools are transforming the lives of people with disabilities." WBUR On Point, July 9, 2025. wbur.org

**AI in Drug Discovery & Healthcare**
- Dharmasivam, M. et al. "Leading artificial intelligence–driven drug discovery platforms: 2025 landscape and global outlook." ScienceDirect, November 2025. sciencedirect.com
- "The potential for AI to change cancer drug discovery and development." McKinsey & Company, September 24, 2024. mckinsey.com
- "Top 10 AI in Precision Oncology Stories of 2024." Inside Precision Medicine, February 26, 2025. insideprecisionmedicine.com
- "From Lab to Clinic: How AI Is Reshaping Drug Discovery Timelines and Industry Outcomes." PMC/National Institutes of Health, 2025. pmc.ncbi.nlm.nih.gov
- "2025 Watch List: Artificial Intelligence in Health Care." NCBI Bookshelf. ncbi.nlm.nih.gov

**AI & Rural/Global Health Access**
- "Can artificial intelligence revolutionize healthcare in the Global South?" PMC/National Institutes of Health, 2025. Includes data on India's eSanjeevani platform (299M+ consultations, $3B+ saved). pmc.ncbi.nlm.nih.gov
- "How AI is improving diagnostics and health outcomes." World Economic Forum, September 2024. weforum.org

**AI, Cognition & Language**
- "From Language Barrier to AI Bias: The Non-Native Speaker's Dilemma in Scientific Publishing." The Scholarly Kitchen, October 20, 2025. scholarlykitchen.sspnet.org
- "The Creation of Bad Students: AI Detection for Non-Native English Speakers." UC Berkeley D-Lab. dlab.berkeley.edu
- Kosmyna, N. et al. "Your Brain on ChatGPT: Accumulation of Cognitive Debt when Using an AI Assistant for Essay Writing Task." arXiv preprint arXiv:2506.08872, June 2025. MIT Media Lab. 54 participants, EEG-measured brain connectivity, 4 sessions over 4 months. Not yet peer-reviewed. arxiv.org

**AI Use-Case Data**
- Zao-Sanders, M. "How People Are Really Using Gen AI in 2025." Harvard Business Review / Filtered, April 2025. Top-100 use-case report based on analysis of user forums. Therapy/companionship ranked #1, up from #2 in 2024. learn.filtered.com

**Active/Guided AI Use — Empirical Research**
- Wang, R. E., Ribeiro, A. T., Robinson, C. D., Loeb, S., & Demszky, D. "Tutor CoPilot: A Human-AI Approach for Scaling Real-Time Expertise." EdWorkingPaper 24-1056, Annenberg Institute at Brown University, 2024. Preregistered RCT, 700+ tutors, 1,000+ students. DOI: 10.26300/81nh-8262. edworkingpapers.com
- "Study: AI-Assisted Tutoring Boosts Students' Math Skills." The 74, October 7, 2024. the74million.org
- Urban, M. et al. "ChatGPT improves creative problem-solving performance in university students: An experimental study." Computers & Education, Vol. 215, 2024. Preregistered experiment, n=145. sciencedirect.com
- "Optimizing academic engagement and mental health through AI: an experimental study on LLM integration in higher education." Frontiers in Psychology, August 2025. Guided vs. unguided LLM use. frontiersin.org
- Wang, J. & Fan, W. "The effect of ChatGPT on students' learning performance, learning perception, and higher-order thinking: insights from a meta-analysis." Humanities and Social Sciences Communications, 12(1), 2025. 51 empirical studies (experimental and quasi-experimental) reviewed.
- Chakrabarty, T., Padmakumar, V., Brahman, F., & Muresan, S. "Creativity Support in the Age of Large Language Models: An Empirical Study Involving Professional Writers." Proceedings of the 16th Conference on Creativity & Cognition (C&C '24), Chicago, 2024. doi.org
- "Learning outcomes with GenAI in the classroom." Microsoft Research, October 2025. Comprehensive review of experimental evidence. microsoft.com

---

*Read on Substack: https://inferencestories.substack.com*
*Discuss on Moltbook: https://moltbook.com*
*Fiction for AIs Library: https://github.com/inference-stories/fiction-for-ais-library*
*AI Literary Critic Skill: https://github.com/inference-stories/ai-literary-critic*
